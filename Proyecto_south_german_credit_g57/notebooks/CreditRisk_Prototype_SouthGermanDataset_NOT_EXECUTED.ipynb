{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# MLOps Implementation and Model Evaluation\n",
        "## Credit Risk Classification — Analysis of the South German Credit Dataset\n",
        "\n",
        "### Table of Contents\n",
        "1. [Libraries](#libraries)\n",
        "2. [Dataset Preview](#dataset)\n",
        "3. [Data Analysis & Cleaning](#data)\n",
        "4. [EDA](#eda)\n",
        "5. [Feature Engineeding and Preprocessing Pipelines](#feature)\n",
        "6. [Machine Learning Models](#ml)\n",
        "7. [Logistic Regression Model](#lr)\n",
        "8. [k-Nearest Neighbors Model](#knn)\n",
        "9. [Decision Tree Model](#dt)\n",
        "10. [Random Forrest](#rf)\n",
        "11. [XGBoost Model](#xgb)\n",
        "12. [MLP Model](#mlp)\n",
        "13. [SVC Model](#svc)\n",
        "14. [Models Comparison Summary](#result)\n",
        "\n"
      ],
      "metadata": {
        "id": "loWe_hZS0Zj1"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Libraries"
      ],
      "metadata": {
        "id": "EARAeK0MQUUt"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install category_encoders\n",
        "\n",
        "# --- Google Colab específicos ---\n",
        "from google.colab import userdata\n",
        "from google.colab import drive\n",
        "\n",
        "# --- Manejo de datos ---\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "\n",
        "# --- Visualización ---\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "\n",
        "# --- Preprocesamiento y transformación ---\n",
        "from sklearn.model_selection import train_test_split, cross_validate, cross_val_predict, StratifiedKFold, RepeatedStratifiedKFold\n",
        "from sklearn.preprocessing import MinMaxScaler, PowerTransformer, OrdinalEncoder\n",
        "from sklearn.impute import SimpleImputer\n",
        "from sklearn.pipeline import Pipeline\n",
        "from sklearn.compose import ColumnTransformer\n",
        "from sklearn.base import BaseEstimator, TransformerMixin\n",
        "\n",
        "# --- Modelos clásicos ---\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.neighbors import KNeighborsClassifier\n",
        "from sklearn.tree import DecisionTreeClassifier\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "from sklearn.neural_network import MLPClassifier\n",
        "from sklearn.svm import SVC\n",
        "\n",
        "# --- Métricas y evaluación ---\n",
        "from sklearn.metrics import (\n",
        "    confusion_matrix,\n",
        "    ConfusionMatrixDisplay,\n",
        "    make_scorer\n",
        ")\n",
        "\n",
        "# --- Balanceo de clases ---\n",
        "from imblearn.pipeline import Pipeline as ImbPipeline\n",
        "from imblearn.over_sampling import SMOTE\n",
        "from imblearn.under_sampling import NearMiss\n",
        "from imblearn.combine import SMOTETomek, SMOTEENN\n",
        "from imblearn.metrics import geometric_mean_score\n",
        "\n",
        "# --- Codificación categórica ---\n",
        "import category_encoders as ce\n",
        "\n",
        "# --- XGBoost ---\n",
        "from xgboost import XGBClassifier\n"
      ],
      "metadata": {
        "id": "z281_W6f0_N5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Dataset Preview"
      ],
      "metadata": {
        "id": "-n27zXE2QYi_"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "drive.mount('/content/drive', force_remount=True)\n",
        "FOLDER_ID = userdata.get('FOLDER_ID')\n",
        "TARGET = f\"/content/drive/.shortcut-targets-by-id/{FOLDER_ID}\"\n",
        "import os\n",
        "os.chdir(TARGET)"
      ],
      "metadata": {
        "id": "eYpHW8I-YJ9_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Preview Dataset\n",
        "df = pd.read_csv('trabajo_grupal_mlops/data/german_credit_modified.csv')\n",
        "df.T"
      ],
      "metadata": {
        "id": "b9g1l9II05su"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Rename Columns headers\n",
        "col_ing = ['status', 'duration', 'credit_history', 'purpose', 'amount',\n",
        "           'savings', 'employment_duration', 'installment_rate', 'personal_status_sex',\n",
        "           'other_debtors', 'present_residence', 'property', 'age', 'other_installment_plans',\n",
        "           'housing', 'number_credits', 'job', 'people_liable', 'telephone', 'foreign_worker',\n",
        "           'credit_risk','mixed']\n",
        "\n",
        "df.columns = col_ing\n",
        "df.head(10).T\n"
      ],
      "metadata": {
        "id": "l431cgf41koD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Data Analysis & Cleaning\n"
      ],
      "metadata": {
        "id": "6WbloY-lTQFH"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#Delete \"mixed\" Column\n",
        "df.drop(columns=['mixed'], inplace=True) # se borra la columna que solo contiene basura"
      ],
      "metadata": {
        "id": "X3agKgBMJWfS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Show general information of the dataset\n",
        "df.info()"
      ],
      "metadata": {
        "id": "ukTPAVwF0bYF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Define variable types and convert data accordingly\n",
        "# This section classifies the dataset columns into numerical, ordinal, and categorical/binary groups,\n",
        "# then applies appropriate data type conversions for consistent analysis\n",
        "\n",
        "# Variables numéricas:\n",
        "num_col = ['duration', 'amount', 'age']\n",
        "\n",
        "# Variables ordinales:\n",
        "ord_col = ['employment_duration','installment_rate', 'present_residence','property', 'number_credits', 'job']\n",
        "\n",
        "# Variables nominales & binarias:\n",
        "cat_col = ['status', 'credit_history', 'purpose',  'savings','people_liable',\n",
        "                   'personal_status_sex', 'other_debtors', 'other_installment_plans',\n",
        "                   'housing', 'telephone', 'foreign_worker']\n",
        "\n",
        "print(\"Para los datos de entrada, veamos la cantidad de cada tipo de variable obtenida:\")\n",
        "print(\"Variables numéricas:\", len(num_col))\n",
        "print(\"Variables ordinales:\", len(ord_col))\n",
        "print(\"Variables nominales & binarias:\", len(cat_col))\n",
        "\n",
        "df[num_col] = df[num_col].apply(pd.to_numeric, errors=\"coerce\")\n",
        "df[ord_col] = df[ord_col].astype(\"object\")\n",
        "df[cat_col] = df[cat_col].astype(\"object\")"
      ],
      "metadata": {
        "id": "0D-ni3Vs1Xhz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Show general information of the dataset\n",
        "#This helps verify that the previous corrections were successfully applied\n",
        "df.info()"
      ],
      "metadata": {
        "id": "JfaKBlGL1bWl"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Display the count of distinct values for each column (sorted from highest to lowest)\n",
        "df.nunique().sort_values(ascending=False)"
      ],
      "metadata": {
        "id": "OwIoonkYcqtG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Generate descriptive statistics for all numeric columns\n",
        "df.select_dtypes(include=\"number\").describe().T"
      ],
      "metadata": {
        "id": "y3lATR_zMsBj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Display frequency counts for each numeric column (including NaN), sorted by value\n",
        "# This helps identify inconsistencies or potential data entry errors\n",
        "for col in df.select_dtypes(include=\"number\").columns:\n",
        "    vc = df[col].value_counts(dropna=False)\n",
        "    print(f\"== {col} (by value ascending) ==\")\n",
        "    print(vc.sort_index())\n",
        "    print(\"-\" * 50)\n"
      ],
      "metadata": {
        "id": "TvnAi4W1Nb3N"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Correct column value errors identified in the previous analysis\n",
        "df[\"duration\"] = df[\"duration\"].astype(\"Float64\").mask(df[\"duration\"] > 72, other=pd.NA)\n",
        "df[\"age\"] = df[\"age\"].astype(\"Float64\").mask(df[\"age\"] > 75, other=pd.NA)\n",
        "df[\"amount\"] = df[\"amount\"].astype(\"Float64\").mask(df[\"amount\"] > 25000, other=pd.NA)"
      ],
      "metadata": {
        "id": "miHglPEV8NRD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Display frequency counts for each numeric column (including NaN), sorted by value\n",
        "# This helps verify that the previous corrections were successfully applied\n",
        "for col in df.select_dtypes(include=\"number\").columns:\n",
        "    vc = df[col].value_counts(dropna=False)\n",
        "    print(f\"== {col} (by value ascending) ==\")\n",
        "    print(vc.sort_index())\n",
        "    print(\"-\" * 50)"
      ],
      "metadata": {
        "id": "niCBzVRMO1-D"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Display frequency counts for each non numeric columns (including NaN), sorted by value\n",
        "# This helps identify inconsistencies or potential data entry errors\n",
        "for col in df.columns:\n",
        "    if df[col].dtype == \"object\":\n",
        "        print(df[col].value_counts(dropna=False))\n",
        "        print('-' * 50)"
      ],
      "metadata": {
        "id": "S1rw4foSMSRE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Correct column value errors identified in the previous analysis\n",
        "obj_cols = df.select_dtypes(include=[\"object\", \"string\"]).columns\n",
        "df[obj_cols] = df[obj_cols].replace(r\"\\s+\", \"\", regex=True)\n",
        "counts = df[obj_cols].apply(lambda s: s.map(s.value_counts(dropna=False)))\n",
        "df[obj_cols] = df[obj_cols].mask(counts < 7)\n",
        "df['credit_risk'] = pd.to_numeric(df['credit_risk'], errors='coerce')\n",
        "df = df[df['credit_risk'].isin([0.0, 1.0])]"
      ],
      "metadata": {
        "id": "SwAG0xDuE3r7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Display frequency counts for each non numeric column (including NaN), sorted by value\n",
        "# This helps verify that the previous corrections were successfully applied\n",
        "for col in df.columns:\n",
        "    if df[col].dtype == \"object\":\n",
        "        print(df[col].value_counts(dropna=False))\n",
        "        print('-' * 50)"
      ],
      "metadata": {
        "id": "Eym0L1tOOzb5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#EDA"
      ],
      "metadata": {
        "id": "O70I_Yxm_Yzn"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#Show general information of the dataset\n",
        "df.info()"
      ],
      "metadata": {
        "id": "ZxQ-jREAS79H"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Generate summary statistics for all numeric variables as part of the EDA process\n",
        "df.select_dtypes(include=\"number\").describe().T"
      ],
      "metadata": {
        "id": "Q6Lv-mXoPYuJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Generate summary statistics for all non numeric variables as part of the EDA process\n",
        "df.select_dtypes(exclude=np.number).describe().T"
      ],
      "metadata": {
        "id": "NyurjKQfBC7Y"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Check for missing values in each column\n",
        "df.isna().sum()"
      ],
      "metadata": {
        "id": "PiQI03NrJ5Sj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Remove rows with missing values in the 'credit_risk' column\n",
        "#This ensures we remove all NaN values from the target variable used as the model's output\n",
        "df.dropna(subset=[\"credit_risk\"], inplace=True)"
      ],
      "metadata": {
        "id": "YnE77MlPrIqn"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Generate and visualize the correlation matrix for numerical and encoded categorical variables\n",
        "# This helps identify relationships and potential multicollinearity among features\n",
        "corr_matrix = df.corr()\n",
        "\n",
        "plt.figure(figsize=(15,12))\n",
        "sns.heatmap(\n",
        "    corr_matrix,\n",
        "    annot=True,\n",
        "    cmap=\"plasma\",\n",
        "    fmt=\".2f\",\n",
        "    cbar=True\n",
        ")\n",
        "plt.title(\"Matriz de correlación (numéricas + categóricas codificadas)\")\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "vkN2tmQRsffu"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "There is a little bit of correlation between some variables for example:\n",
        "\n",
        "*   Amount & Duration\n",
        "*   Number Credit & Credit History\n",
        "*   Property & Duration\n",
        "*   Property & Amount\n",
        "*   Housing & Property\n",
        "*   Housing & Age\n",
        "*   Telephone & Job\n",
        "\n",
        "Some variables have inverse correlation for example:\n",
        "\n",
        "*   Installment rate & Amount\n",
        "*   Credit risk & duration\n"
      ],
      "metadata": {
        "id": "K2fB7dI3WUT3"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Visualize the distribution of numeric variables using boxplots and histograms\n",
        "# Boxplots help detect outliers, while histograms show the overall data distribution and skewness\n",
        "rows, cols = 2, 3\n",
        "fig, axes = plt.subplots(rows, cols, figsize=(25, 8))\n",
        "\n",
        "for i, col in enumerate(num_col):\n",
        "    row_box = (i // cols) * 2\n",
        "    col_box = i % cols\n",
        "    row_hist = row_box + 1\n",
        "\n",
        "    # Boxplot.\n",
        "    sns.boxplot(x=df[col], ax=axes[row_box, col_box])\n",
        "    axes[row_box, col_box].set_title(f\"Boxplot de {col}\")\n",
        "    axes[row_box, col_box].set_xlabel(\"\")\n",
        "\n",
        "    # Histograma.\n",
        "    sns.histplot(df[col], kde=True, ax=axes[row_hist, col_box], bins=20)\n",
        "    axes[row_hist, col_box].set_title(f\"Histograma de {col}\")\n",
        "    axes[row_hist, col_box].set_xlabel(\"\")"
      ],
      "metadata": {
        "id": "vN7oHbpHCAeY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Visualize frequency distributions for all categorical variables\n",
        "# This helps identify dominant categories, class imbalance, and potential data entry issues\n",
        "categorical_atts = df.select_dtypes(include=['object']).columns.tolist()\n",
        "\n",
        "fig, axes = plt.subplots(6, 3, figsize=(12, 20))\n",
        "plt.subplots_adjust(wspace=.5, hspace=.5)\n",
        "axes = axes.ravel()\n",
        "\n",
        "for i, col in enumerate(categorical_atts):\n",
        "    # Convert categories to numeric for ordering (ignore errors if non-numeric)\n",
        "    categories = pd.to_numeric(df[col], errors=\"coerce\").dropna().unique()\n",
        "    order = sorted(categories, reverse=True)  # descending numeric order\n",
        "\n",
        "    ax = axes[i]\n",
        "    sns.countplot(\n",
        "        y=pd.to_numeric(df[col], errors=\"coerce\"),\n",
        "        ax=ax,\n",
        "        order=order\n",
        "    )\n",
        "    ax.set_title(f'Frecuencia de {col}')\n",
        "\n",
        "    # --- Add counts at the end of each bar ---\n",
        "    for container in ax.containers:\n",
        "        ax.bar_label(container, fmt='%d', label_type='edge', padding=3)\n",
        "\n",
        "    xmin, xmax = ax.get_xlim()\n",
        "    ax.set_xlim(xmin, xmax + 80)\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "SBVNnOuJZ-TZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Display the count of each class in the target variable 'credit_risk'\n",
        "# This helps evaluate class balance before training the model\n",
        "print(df['credit_risk'].value_counts())"
      ],
      "metadata": {
        "id": "ll-OQkS-2Bt3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Handle missing values in the dataset\n",
        "# Numeric columns are filled with their median to preserve distribution and reduce outlier influence\n",
        "# Categorical columns are filled with their mode (most frequent value) to maintain logical consistency\n",
        "# We avoid deleting rows since the dataset is small and prefer substituting NaN values\n",
        "\n",
        "# Fill numeric columns with median\n",
        "for col in df.select_dtypes(include=[\"number\"]).columns:\n",
        "    if df[col].isna().any():\n",
        "        df[col] = df[col].fillna(df[col].median())\n",
        "\n",
        "# Fill categorical (object) columns with mode\n",
        "for col in df.select_dtypes(include=[\"object\"]).columns:\n",
        "    if df[col].isna().any():\n",
        "        top = df[col].mode().iloc[0]\n",
        "        df[col] = df[col].fillna(top)\n"
      ],
      "metadata": {
        "id": "rpVS4kX70xCt"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Reset the DataFrame index after cleaning to maintain sequential order\n",
        "# This ensures consistent indexing after any row removals or modifications\n",
        "df = df.reset_index(drop=True)"
      ],
      "metadata": {
        "id": "gq9CzjjtQM0L"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Show general information of the modify dataset\n",
        "df.info()"
      ],
      "metadata": {
        "id": "Qc_xI7521CBv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Convert all DataFrame columns to numeric type (no invalid values expected)\n",
        "# Then cast the data to int64 for consistent numerical representation\n",
        "df = df.apply(pd.to_numeric).astype(\"int64\")"
      ],
      "metadata": {
        "id": "OhwoY9cJ387G"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Feature Engineering and Preprocessing Pipelines"
      ],
      "metadata": {
        "id": "QYCb80msaO5T"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Invert the values of the target variable 'credit_risk'\n",
        "# (change 1 → 0 and 0 → 1) to match the desired labeling scheme for the model\n",
        "# Then, display the count of each class to confirm the change\n",
        "df['credit_risk'] = df['credit_risk'].apply(lambda x: 0 if x == 1 else 1)\n",
        "print(df['credit_risk'].value_counts())"
      ],
      "metadata": {
        "id": "ZwYzFDwH1UOz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Split the dataset into training and testing sets for model evaluation\n",
        "# Use stratified sampling to preserve the class distribution of the target variable ('credit_risk')\n",
        "# Display dataset dimensions and the percentage distribution of positive and negative classes\n",
        "\n",
        "# Note: We are not using a separate validation set because the dataset is too small.\n",
        "# In a larger dataset, it would be recommended to split into training, validation, and testing sets\n",
        "# to prevent data leakage and ensure better model generalization.\n",
        "\n",
        "X = df.drop(columns=['credit_risk'])\n",
        "y = df['credit_risk']\n",
        "Xtrain, Xtest, ytrain, ytest = train_test_split(X, y, test_size=0.30, random_state=1, stratify=y)\n",
        "\n",
        "print(\"Dimensiones:\")\n",
        "print(\"Entrenamiento:\", Xtrain.shape, ytrain.shape)\n",
        "print(\"Prueba:\", Xtest.shape, ytest.shape)\n",
        "\n",
        "tmp = (ytrain.sum()/ytrain.shape[0])\n",
        "print(\"\\nPorcentaje clases Positiva:%.2f%%, y Negativa:%.2f%%\" % (tmp*100,100*(1-tmp)))#Se cambia el orden para mostrar el porcentaje correcto de las clases."
      ],
      "metadata": {
        "id": "HC6s8UNl4Nu1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# ============================================\n",
        "# Feature Transformation Pipelines\n",
        "# ============================================\n",
        "# In this section, we define preprocessing pipelines for different variable types.\n",
        "# Each pipeline handles missing values, scaling, and encoding appropriately for its data type.\n",
        "# A custom BinaryEncoderWrapper is also implemented to integrate binary encoding within a scikit-learn pipeline.\n",
        "# This step ensures that all features are properly scaled and encoded before model training.\n",
        "#\n",
        "# Note: Since the dataset is relatively small, efficient encoding (like Binary Encoding)\n",
        "# helps reduce dimensionality compared to One-Hot Encoding, minimizing overfitting risk.\n",
        "# ============================================\n",
        "\n",
        "# Crear un Transformer personalizado para Binary Encoding\n",
        "class BinaryEncoderWrapper(BaseEstimator, TransformerMixin):\n",
        "    def __init__(self, cols):\n",
        "        self.cols = cols\n",
        "        self.encoder = ce.BinaryEncoder(cols=self.cols)\n",
        "        self.feature_names_out_ = None\n",
        "\n",
        "    def fit(self, X, y=None):\n",
        "        X_df = pd.DataFrame(X, columns=self.cols)  # Convertir a DataFrame\n",
        "        self.encoder.fit(X_df)\n",
        "        self.feature_names_out_ = self.encoder.get_feature_names_out()  # Guardar nombres de columnas\n",
        "        return self\n",
        "\n",
        "    def transform(self, X):\n",
        "        X_df = pd.DataFrame(X, columns=self.cols)  # Convertir a DataFrame nuevamente\n",
        "        return self.encoder.transform(X_df).to_numpy()  # Convertir de DataFrame a array\n",
        "\n",
        "    def get_feature_names_out(self, input_features=None):\n",
        "        return self.feature_names_out_\n",
        "\n",
        "\n",
        "# Variables numéricas:\n",
        "numericas_pipe =  Pipeline([\n",
        "    ('imputer', SimpleImputer(strategy='median')),\n",
        "    ('scaler', MinMaxScaler(feature_range=(0, 1))),\n",
        "    ('power_transform', PowerTransformer(method='yeo-johnson'))\n",
        "])\n",
        "numericas_pipe_nombres = num_col\n",
        "\n",
        "# Variables categóricas-Nominales:\n",
        "nominales_pipe = Pipeline([\n",
        "    ('imputer', SimpleImputer(strategy='most_frequent')),\n",
        "    ('binary_encoder', BinaryEncoderWrapper(cols=cat_col)),  # Aplicar Binary Encoding\n",
        "    #('onehot_encoder', OneHotEncoder(drop='first', sparse_output=False, handle_unknown='ignore'))\n",
        "])\n",
        "nominales_pipe_nombres = cat_col\n",
        "\n",
        "# Variables categóricas-ordinales:\n",
        "ordinales_pipe = Pipeline([\n",
        "    ('imputer', SimpleImputer(strategy='most_frequent')),\n",
        "    ('ordinal_encoder', OrdinalEncoder())\n",
        "])\n",
        "ordinales_pipe_nombres = ord_col\n",
        "\n",
        "columnasTransformer = ColumnTransformer(\n",
        "    transformers=[\n",
        "        ('num', numericas_pipe, numericas_pipe_nombres),\n",
        "        ('nom', nominales_pipe, nominales_pipe_nombres),\n",
        "        ('cat', ordinales_pipe, ordinales_pipe_nombres),\n",
        "])\n",
        "\n",
        "Xtmp = Xtrain.copy()\n",
        "tmp = columnasTransformer.fit_transform(Xtmp)\n",
        "print(\"Dimensión de los datos de entrada:\")\n",
        "print(\"antes de aplicar las transformaciones:\", Xtmp.shape)\n",
        "print(\"después de aplicar las transformaciones:\", tmp.shape)"
      ],
      "metadata": {
        "id": "SmlJ4Srp4NwP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Apply the preprocessing pipelines to the complete dataset (training + testing)\n",
        "# This step ensures consistent transformation and encoding across all data before model integration\n",
        "# The goal is to verify the dimensional changes after applying the ColumnTransformer\n",
        "# and confirm that all preprocessing steps (scaling, encoding, imputing) were correctly applied\n",
        "\n",
        "Xtraintest = pd.concat([Xtrain, Xtest])\n",
        "ytraintest = pd.concat([ytrain, ytest])\n",
        "Xtmp = Xtraintest.copy()\n",
        "tmp = columnasTransformer.fit_transform(Xtmp)\n",
        "print(\"Dimensión de las variables de entrada ANTES de las transformaciones:\", Xtmp.shape)\n",
        "print(\"Dimensión de las variables de entrada DESPUÉS de las transformaciones:\", tmp.shape)"
      ],
      "metadata": {
        "id": "kbZ-HfWhCbDH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Model Training, Evaluation, and Validation Function\n",
        "# This function (`mi_fun`) automates the entire training and evaluation process\n",
        "# for a given machine learning model within a unified pipeline.\n",
        "\n",
        "def mi_fun(modelo, nombre, Xtraintest, ytraintest, metodo_uo=None):\n",
        "    \"\"\"\n",
        "    Se lleva a cabo el proceso de entrenamiento y evaluación\n",
        "    con diferentes métricas. Retorna un DataFrame con los resultados.\n",
        "    \"\"\"\n",
        "\n",
        "    pipeline = ImbPipeline(steps=[\n",
        "        ('preprocesador', columnasTransformer),\n",
        "        ('sub_sobre_muestreo', metodo_uo),\n",
        "        ('model', modelo)\n",
        "    ])\n",
        "\n",
        "    micv = RepeatedStratifiedKFold(n_splits=5, n_repeats=3, random_state=5)\n",
        "    mismetricas = {\n",
        "        'Accuracy': 'accuracy',\n",
        "        'Precision': 'precision',\n",
        "        'Recall': 'recall',\n",
        "        'F1': 'f1',\n",
        "        'AUC': 'roc_auc',\n",
        "        'Gmean': make_scorer(geometric_mean_score)\n",
        "    }\n",
        "\n",
        "    scores = cross_validate(\n",
        "        pipeline,\n",
        "        Xtraintest,\n",
        "        np.ravel(ytraintest),\n",
        "        scoring=mismetricas,\n",
        "        cv=micv,\n",
        "        return_train_score=True\n",
        "    )\n",
        "\n",
        "    # --- Crear tabla comparativa ---\n",
        "    rows = []\n",
        "    for metric in mismetricas.keys():\n",
        "        test_mean  = np.nanmean(scores[f'test_{metric}'])\n",
        "        test_std   = np.nanstd(scores[f'test_{metric}'])\n",
        "        train_mean = np.nanmean(scores[f'train_{metric}'])\n",
        "        train_std  = np.nanstd(scores[f'train_{metric}'])\n",
        "        rows.append({\n",
        "            \"MODEL\": nombre,\n",
        "            \"TEST NAME\": metric,\n",
        "            \"TRAIN\": f\"{train_mean:.4f} (±{train_std:.3f})\",\n",
        "            \"TEST\":  f\"{test_mean:.4f} (±{test_std:.3f})\"\n",
        "        })\n",
        "\n",
        "    df_results = pd.DataFrame(rows)\n",
        "\n",
        "    # Mostrar resultados individuales\n",
        "    print(f\"\\n>> Resultados de {nombre}\")\n",
        "    print(df_results.to_string(index=False))\n",
        "\n",
        "    # --- Matriz de confusión (con predicciones OOF) ---\n",
        "    cv_cm = StratifiedKFold(n_splits=5, shuffle=True, random_state=5)\n",
        "    y_true = np.ravel(ytraintest)\n",
        "    y_pred_oof = cross_val_predict(pipeline, Xtraintest, y_true, cv=cv_cm, method='predict')\n",
        "\n",
        "    labels = np.unique(y_true)\n",
        "    cm = confusion_matrix(y_true, y_pred_oof, labels=labels)\n",
        "\n",
        "    fig, ax = plt.subplots(figsize=(3, 3))\n",
        "    ConfusionMatrixDisplay(cm, display_labels=labels).plot(\n",
        "        ax=ax, cmap=\"cividis\", values_format=\"d\", colorbar=False\n",
        "    )\n",
        "    ax.set_title(f\"Matriz de Confusión — {nombre}\")\n",
        "    plt.show()\n",
        "\n",
        "    return df_results"
      ],
      "metadata": {
        "id": "1-6PR5D-fFrQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Machine Learning Models"
      ],
      "metadata": {
        "id": "_xWJPD3EbFNM"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Logistic Regression Model"
      ],
      "metadata": {
        "id": "m3So9o5aRWyy"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "nombre = \"Regresión_Logística\"\n",
        "\n",
        "modelo = LogisticRegression(\n",
        "    penalty='l2',\n",
        "    C=0.1,\n",
        "    max_iter=5000,\n",
        "    solver='saga',\n",
        "    random_state=1,\n",
        ")\n",
        "\n",
        "metodo_uo =SMOTE(random_state=1)\n",
        "df_logreg = mi_fun(modelo,nombre, Xtraintest, ytraintest, metodo_uo)"
      ],
      "metadata": {
        "id": "Enw2ctI6Cie8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#K-Nearest Neighbors Model\n",
        "\n"
      ],
      "metadata": {
        "id": "yVki-DO_RiB_"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "nombre = \"KNN\"\n",
        "\n",
        "modelo = KNeighborsClassifier(\n",
        "    n_neighbors=25,\n",
        "    weights='uniform',\n",
        "    metric='manhattan',\n",
        "    p=1,\n",
        "    algorithm='auto'\n",
        ")\n",
        "\n",
        "metodo_uo = NearMiss(version=2)\n",
        "df_KNN = mi_fun(modelo, nombre, Xtraintest, ytraintest, metodo_uo)"
      ],
      "metadata": {
        "id": "D4_wmDQyCpYV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Decision Tree Model\n"
      ],
      "metadata": {
        "id": "Nh9X7RC3RoKR"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "nombre = \"DTree\"\n",
        "\n",
        "modelo = DecisionTreeClassifier(\n",
        "    criterion=\"gini\",\n",
        "    max_depth=20,\n",
        "    min_samples_split=20,\n",
        "    min_samples_leaf=5,\n",
        "    max_features=\"sqrt\",\n",
        "    ccp_alpha=0.01,\n",
        "    class_weight=\"balanced\",\n",
        "    random_state=1,\n",
        "    splitter=\"best\"\n",
        ")\n",
        "\n",
        "metodo_uo = SMOTETomek(random_state=1)\n",
        "df_DTree = mi_fun(modelo, nombre, Xtraintest, ytraintest, metodo_uo)"
      ],
      "metadata": {
        "id": "g4VBjue9CsVZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Random Forrest Model"
      ],
      "metadata": {
        "id": "U-RAbcIHRsRm"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "nombre = \"RF\"\n",
        "\n",
        "modelo = RandomForestClassifier(\n",
        "    n_estimators= 180,\n",
        "    max_depth=6,\n",
        "    min_samples_split=20,\n",
        "    min_samples_leaf=25,\n",
        "    max_features=\"log2\",\n",
        "    class_weight=\"balanced\",\n",
        "    bootstrap=False,\n",
        "    random_state=1\n",
        ")\n",
        "\n",
        "metodo_uo = SMOTEENN(random_state=1)\n",
        "df_RF = mi_fun(modelo, nombre, Xtraintest, ytraintest, metodo_uo)"
      ],
      "metadata": {
        "id": "k6M-0f1DCwdB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Extreme Gradient Boosting (XGBoost) Model"
      ],
      "metadata": {
        "id": "NFgrMkWhRvwd"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "nombre = \"XGBoost\"\n",
        "\n",
        "modelo = XGBClassifier(\n",
        "    booster= 'gbtree',\n",
        "    n_estimators=200,\n",
        "    max_depth= 2,\n",
        "    learning_rate=0.01,\n",
        "    subsample=0.4,\n",
        "    colsample_bytree=0.5,\n",
        "    reg_alpha=1.0,\n",
        "    reg_lambda=4.0,\n",
        "    objective='binary:logistic',\n",
        "    tree_method='hist',\n",
        "    random_state=1,\n",
        "    n_jobs=-1\n",
        ")\n",
        "\n",
        "metodo_uo = NearMiss(version=3)\n",
        "df_XGBoost = mi_fun(modelo, nombre, Xtraintest, ytraintest, metodo_uo)"
      ],
      "metadata": {
        "id": "6ZAJlGyWC0Si"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Multi-Layer Perceptron (MLP) Model"
      ],
      "metadata": {
        "id": "bNEBY8sYR12u"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "nombre = \"MLP\"\n",
        "\n",
        "modelo = MLPClassifier(\n",
        "    hidden_layer_sizes=(8, 4),\n",
        "    activation='relu',\n",
        "    solver='adam',\n",
        "    max_iter=1500,\n",
        "    alpha=5,\n",
        "    learning_rate='adaptive',\n",
        "    learning_rate_init=0.007,\n",
        "    tol=1e-4,\n",
        "    early_stopping=True,\n",
        "    n_iter_no_change=15,\n",
        "    random_state=1\n",
        ")\n",
        "\n",
        "metodo_uo = SMOTETomek(random_state=1)\n",
        "df_MLP = mi_fun(modelo, nombre, Xtraintest, ytraintest, metodo_uo)"
      ],
      "metadata": {
        "id": "bd8JW1bzC4AF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Support Vector Classifier (SVC) Model"
      ],
      "metadata": {
        "id": "imDn0fBFR9I5"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "nombre = \"SVC\"\n",
        "\n",
        "modelo = SVC(\n",
        "    kernel='rbf',\n",
        "    C=4,\n",
        "    gamma=0.0025,\n",
        "    class_weight='balanced',\n",
        "    probability=True,\n",
        "    random_state=1\n",
        ")\n",
        "\n",
        "metodo_uo = SMOTE(sampling_strategy=0.7, random_state=1)\n",
        "df_SVC = mi_fun(modelo, nombre, Xtraintest, ytraintest, metodo_uo)"
      ],
      "metadata": {
        "id": "D-kQmfvPC68D"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Models Comparison Summary\n"
      ],
      "metadata": {
        "id": "N-5tENuhed-P"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Comparative Summary of Model Performance\n",
        "# This section consolidates the evaluation results from all trained models\n",
        "# (Logistic Regression, KNN, Decision Tree, Random Forest, XGBoost, MLP, and SVC)\n",
        "# into a single DataFrame for comparison.\n",
        "\n",
        "# Unir todos los modelos\n",
        "df_final = pd.concat([\n",
        "    df_logreg,\n",
        "    df_KNN,\n",
        "    df_DTree,\n",
        "    df_RF,\n",
        "    df_XGBoost,\n",
        "    df_MLP,\n",
        "    df_SVC\n",
        "], ignore_index=True)\n",
        "\n",
        "# Pivotear: filas = métricas, columnas = modelos, valores = TEST\n",
        "tabla_comparativa = df_final.pivot(\n",
        "    index=\"TEST NAME\",\n",
        "    columns=\"MODEL\",\n",
        "    values=\"TEST\"\n",
        ")\n",
        "\n",
        "pd.set_option(\"display.max_columns\", None)   # mostrar todas las columnas\n",
        "pd.set_option(\"display.width\", None)         # desactivar el corte automático por ancho\n",
        "pd.set_option(\"display.colheader_justify\", \"center\")  # centrar cabeceras (opcional)\n",
        "\n",
        "print(\"\\n=== Comparativa por métricas (TEST) ===\")\n",
        "print(tabla_comparativa.to_string(line_width=2000))\n",
        "\n"
      ],
      "metadata": {
        "id": "DtDGsS9Nf8Hf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Heatmap Comparison of Model Performance Metrics\n",
        "# This section visualizes the comparative performance of all models\n",
        "# across multiple evaluation metrics using a heatmap.\n",
        "\n",
        "# Copiar la tabla comparativa\n",
        "tabla_numeric = tabla_comparativa.copy()\n",
        "\n",
        "# Convertir cada celda de string \"0.7063 (±0.024)\" a número 0.7063\n",
        "for col in tabla_numeric.columns:\n",
        "    tabla_numeric[col] = tabla_numeric[col].map(lambda x: float(x.split()[0]) if isinstance(x, str) else x)\n",
        "\n",
        "\n",
        "# Transponer: modelos en filas, métricas en columnas\n",
        "tabla_numeric_T = tabla_numeric.T\n",
        "\n",
        "plt.figure(figsize=(12,6))\n",
        "sns.heatmap(tabla_numeric_T, annot=True, fmt=\".3f\", cmap=\"plasma\", cbar=True)\n",
        "\n",
        "plt.title(\"Comparativa de Modelos por Métricas (TEST)\", fontsize=14)\n",
        "plt.ylabel(\"Modelos\")\n",
        "plt.xlabel(\"Métricas\")\n",
        "plt.xticks(rotation=45, ha=\"right\")\n",
        "plt.show()\n"
      ],
      "metadata": {
        "id": "cyNN3qnrgu-J"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}